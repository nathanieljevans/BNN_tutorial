{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks \n",
    "\n",
    "This script explores using pytorch and pyro to build, train and test a bayesian neural network. The advantage and novelty of this model is that inference is done probablistically; thereby allowing us to quantify the certainty of model results. Kind of like a built in sensitivity  analysis. \n",
    "\n",
    "I like the way [this](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd) [3] tutorial phrases it:  \n",
    "\n",
    "> Making Your Neural Network Say “I Don’t Know”\n",
    "\n",
    "In this tutorial, we'll go over \n",
    "- key differences of `Bayesian Neural Networks (BNN)` vs `Neural Networks (NN)` \n",
    "- how BNNs do inference & training \n",
    "- simple tutorial on how to build a BNN using `pytorch` and `pyro` \n",
    "- difference in training and prediction time complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNNs vs NNs \n",
    "\n",
    "## Classic Neural Network \n",
    "\n",
    "\n",
    "## Bayesian Neural Network \n",
    "\n",
    "![image.png](https://www.researchgate.net/profile/Florian_Haese/publication/329843608/figure/fig2/AS:713727343067138@1547177267367/llustration-of-a-Bayesian-Neural-Network-BNN-A-A-Bayesian-neuron-defines-a_W640.jpg)\n",
    "Taken from [1]. \n",
    "\n",
    "\n",
    "# BNN advantage\n",
    "\n",
    "\n",
    "## What happens when you give cutting edge neural network an image of noise? \n",
    "\n",
    "![image.png](https://miro.medium.com/max/802/0*HG51qQU8I34_fUgB.jpg)\n",
    "\n",
    "--- \n",
    "\n",
    "The most obvious advantage of these BNNs are that they allow the model to propogate it's uncertainty about a prediction, from [this]( https://krasserm.github.io/2019/03/14/bayesian-neural-networks/) tutorial, there is a great plot that shows this.\n",
    "\n",
    "![image.png](https://krasserm.github.io/img/2019-03-14/output_9_1.png)\n",
    "\n",
    "From this we can see that the BNN knows where in it's feature space it has accurate predictions, and where it does not. I've been thinking about this as a built in sensitivity analysis. With a few extra lines of code, we can have the model say, \"Hey, I don't know what the answer is, don't trust me\", which is a key feature when making important decisions. \n",
    "\n",
    "Other advantages include being able to include priors on our weight distributions (could be tough to know how to do this well). Also priors on our X distributions, which could be as simple as calculating p(x) over each mini-batch of the training set and using this as the prior on test sets. \n",
    "\n",
    "# Training a BNN \n",
    "\n",
    "## Variation Inference \n",
    "\n",
    "\n",
    "## Sampling \n",
    "\n",
    "\n",
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.distributions import constraints\n",
    "import torchvision as torchv\n",
    "import torchvision.transforms as torchvt\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch import nn\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "import pyro\n",
    "from pyro import poutine\n",
    "import pyro.optim as pyroopt\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.bnn as bnn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.distributions.utils import lazy_property\n",
    "import math\n",
    "from torch.utils import data\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our data in \n",
    "\n",
    "We'll use the classic `Iris` Dataset.\n",
    "\n",
    "number of features: 4 \n",
    "number of observations: 150\n",
    "number of classes: 3 \n",
    "\n",
    "We'll save each observation separately to disk as a tensor object (.pt), so that we can build an efficient DataLoader (for learning purposes, since our volume doesn't actually make this necessary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_iris(return_X_y=True)\n",
    "\n",
    "for i,x in enumerate(X):\n",
    "    torch.save(torch.tensor(x).float(), './data/%d.pt' %i)\n",
    "\n",
    "n_classes=len(set(Y)) # torch.tensor(len(set(Y))).int()\n",
    "print(f'Number of classes: {n_classes}')\n",
    "\n",
    "partition = {'train':[str(x) for x in range(0,100)],\n",
    "             'val':[str(x) for x in range(100,125)],\n",
    "             'test':[str(x) for x in range(125,150)]}\n",
    "\n",
    "labels = {str(i):torch.tensor(j).to(torch.int64) for i,j in zip(range(150), Y)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class: first part of our DataLoader \n",
    "\n",
    "We have to define a dataset class, which is where we can tell the dataloader where to look for each observation (key -> path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data, load from disk'\n",
    "        ID = self.list_IDs[index]\n",
    "        X = torch.load('data/' + ID + '.pt')\n",
    "        y = self.labels[ID]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # CUDA for PyTorch\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "        def get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels)\n",
    "train_loader = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['val'], labels)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "test_set = Dataset(partition['test'], labels)\n",
    "test_generator = data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, n_classes=n_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Dropout(p=0.2),\n",
    "                                nn.Linear(4, 20),\n",
    "                                nn.Dropout(p=0.9),\n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(20,50),\n",
    "                                nn.Dropout(p=0.9),\n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(50, 20),\n",
    "                                nn.Dropout(p=0.9),\n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(20, n_classes),\n",
    "                                nn.LogSoftmax(dim=-1))\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.fc(inp)\n",
    "\n",
    "FC_NN = FCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freeze_support' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-4b551d847854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfreeze_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moptim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFC_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'freeze_support' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    optim = SGD(FC_NN.parameters(recurse=True), lr=0.1, momentum=0.95)\n",
    "    epochs = 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0.0 \n",
    "        total = 0.0\n",
    "        correct = 0.0\n",
    "        for x, y in train_loader:\n",
    "\n",
    "            FC_NN.zero_grad()\n",
    "            pred = FC_NN.forward(x.cuda())\n",
    "            loss = nnf.binary_cross_entropy_with_logits(pred, nnf.one_hot(y.cuda(), n_classes).float())\n",
    "            total_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (pred.argmax(-1) == y.cuda()).sum().item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        print('epoch: %d | loss: %.3f | acc: %.5f' %((i+1), total_loss, correct/total*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "1.How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry. ResearchGate https://www.researchgate.net/publication/329843608_How_machine_learning_can_assist_the_interpretation_of_ab_initio_molecular_dynamics_simulations_and_conceptual_understanding_of_chemistry/figures?lo=1.\n",
    "\n",
    "2.Variational inference for Bayesian neural networks - Martin Krasser’s Blog. https://krasserm.github.io/2019/03/14/bayesian-neural-networks/.\n",
    "\n",
    "3.Chopra, P. Making Your Neural Network Say “I Don’t Know” — Bayesian NNs using Pyro and PyTorch. Medium https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd (2019).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
